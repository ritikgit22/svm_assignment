{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Q1 - What is a Support Vector Machine (SVM)?\n",
        "\n",
        "Ans - SVM is a supervised learning algorithm used for classification and regression tasks. It finds the optimal hyperplane that best separates different classes in a dataset.\n",
        "\n",
        "Q2. What is the difference between Hard Margin and Soft Margin SVM?\n",
        "\n",
        "Ans - Hard Margin SVM strictly separates classes with a hyperplane, allowing no misclassification but requiring data to be linearly separable.\n",
        "Soft Margin SVM allows some misclassification by introducing a penalty (C parameter) to handle noise and overlapping data.\n",
        "\n",
        "Q3. What is the mathematical intuition behind SVM?\n",
        "\n",
        "Ans - SVM maximizes the margin between the closest points of different classes (support vectors) while minimizing classification error using a cost function.\n",
        "\n",
        "Q4 - What is the role of Lagrange Multipliers in SVM?\n",
        "\n",
        "Ans - Lagrange multipliers are used to transform SVM optimization into a constrained problem, making it easier to solve using quadratic programming.\n",
        "\n",
        "Q5 - What are Support Vectors in SVM?\n",
        "\n",
        "Ans - Support vectors are the data points closest to the decision boundary, determining the hyperplane's position and orientation.\n",
        "\n",
        "Q6 - What is a Support Vector Classifier (SVC)?\n",
        "\n",
        "Ans - SVC is an SVM model used for classification tasks.\n",
        "\n",
        "Q7 - What is a Support Vector Regressor (SVR)?\n",
        "\n",
        "Ans- SVR is an SVM model used for regression, predicting continuous values while maintaining a margin of tolerance.\n",
        "\n",
        "Q8 - What is the Kernel Trick in SVM?\n",
        "\n",
        "Ans - The Kernel Trick maps input data into a higher-dimensional space to make it linearly separable without explicitly computing transformations.\n",
        "\n",
        "Q9 - Compare Linear Kernel, Polynomial Kernel, and RBF Kernel.\n",
        "\n",
        "Ans - Linear Kernel: Best for linearly separable data.\n",
        "Polynomial Kernel: Captures non-linearity but may be computationally expensive.\n",
        "RBF Kernel: Most commonly used as it works well with non-linear relationships.\n",
        "\n",
        "Q10 - What is the effect of the C parameter in SVM?\n",
        "\n",
        "Ans - The C parameter controls the trade-off between maximizing margin and minimizing classification error. A higher C values focus more on correct classification.\n",
        "\n",
        "Q11 - What is the role of the Gamma parameter in RBF Kernel SVM?\n",
        "Gamma defines how far the influence of a single training point reaches. Higher gamma results in more complex models, while lower gamma makes a simpler model.\n",
        "\n",
        "Q12 - What is the Naïve Bayes classifier, and why is it called \"Naïve\"?\n",
        "\n",
        "Ans - It is a probabilistic classifier based on Bayes' theorem, assuming that features are independent given the class label, hence \"Naïve.\"\n",
        "\n",
        "Q13 - What is Bayes’ Theorem?\n",
        "\n",
        "Ans - Bayes' theorem states that the probability of a class given the data is proportional to the prior probability of the class times the likelihood of the data given the class.\n",
        "\n",
        "Q14 - Explain the differences between Gaussian Naïve Bayes, Multinomial Naïve Bayes, and Bernoulli Naïve Bayes.\n",
        "\n",
        "Ans - Gaussian Naïve Bayes: Assumes features are normally distributed.\n",
        "Multinomial Naïve Bayes: Used for text classification with word counts.\n",
        "Bernoulli Naïve Bayes: Works with binary data (presence or absence of features).\n",
        "\n",
        "Q15 - When should you use Gaussian Naïve Bayes over other variants?\n",
        "Ans - When the dataset features are continuous and normally distributed.\n",
        "\n",
        "Q16 - What are the key assumptions made by Naïve Bayes?\n",
        "\n",
        "Ans - Features are independent.\n",
        "All features contribute equally.\n",
        "The prior probabilities of classes are correct.\n",
        "\n",
        "Q17 - What are the advantages and disadvantages of Naïve Bayes?\n",
        "\n",
        "Ans - Advantages: Simple, fast, works well with small datasets and text classification.\n",
        "Disadvantages: Assumes feature independence, which is rarely true.\n",
        "\n",
        "Q18 - Why is Naïve Bayes a good choice for text classification?\n",
        "Ans - Because of its efficiency and ability to handle high-dimensional data effectively.\n",
        "\n",
        "Q19 - Compare SVM and Naïve Bayes for classification tasks:\n",
        "\n",
        "Ans - SVM is better for complex and large-scale datasets with non-linearity.\n",
        "Naïve Bayes is faster and works well for text classification.\n",
        "\n",
        "Q20 - How does Laplace Smoothing help in Naïve Bayes?\n",
        "\n",
        "Ans - It prevents zero probability issues by adding a small smoothing factor (like +1) to all probabilities."
      ],
      "metadata": {
        "id": "byipGzi-Nq4b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q21 - Write a Python program to train an SVM Classifier on the Iris dataset and evaluate accuracy."
      ],
      "metadata": {
        "id": "mZ2ifSigPcKK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "iris = datasets.load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train SVM classifier\n",
        "svm_clf = SVC(kernel='linear')\n",
        "svm_clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate accuracy\n",
        "y_pred = svm_clf.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")"
      ],
      "metadata": {
        "id": "rMwH2KzcQcUM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q22 - Write a Python program to train two SVM classifiers with Linear and RBF kernels on the Wine dataset, then compare their accuracies."
      ],
      "metadata": {
        "id": "pZiGw2HQQjJg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "wine = datasets.load_wine()\n",
        "X, y = wine.data, wine.target\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train SVM classifiers with different kernels\n",
        "svm_linear = SVC(kernel='linear')\n",
        "svm_rbf = SVC(kernel='rbf')\n",
        "\n",
        "svm_linear.fit(X_train, y_train)\n",
        "svm_rbf.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred_linear = svm_linear.predict(X_test)\n",
        "y_pred_rbf = svm_rbf.predict(X_test)\n",
        "\n",
        "# Compare accuracies\n",
        "accuracy_linear = accuracy_score(y_test, y_pred_linear)\n",
        "accuracy_rbf = accuracy_score(y_test, y_pred_rbf)\n",
        "\n",
        "print(f\"Linear Kernel Accuracy: {accuracy_linear:.2f}\")\n",
        "print(f\"RBF Kernel Accuracy: {accuracy_rbf:.2f}\")"
      ],
      "metadata": {
        "id": "eVVj8RvkQkrl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q23 - Write a Python program to train an SVM Regressor (SVR) on a housing dataset and evaluate it using Mean Squared Error (MSE)."
      ],
      "metadata": {
        "id": "dRSRL6ySQqoY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load dataset\n",
        "housing = datasets.fetch_california_housing()\n",
        "X, y = housing.data, housing.target\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train SVR model\n",
        "svr = SVR(kernel='rbf')\n",
        "svr.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate MSE\n",
        "y_pred = svr.predict(X_test)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Mean Squared Error: {mse:.2f}\")"
      ],
      "metadata": {
        "id": "ofV1d9EmQwrb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q24 -  Write a Python program to train an SVM Classifier with a Polynomial Kernel and visualize the decision boundary."
      ],
      "metadata": {
        "id": "oH_rRiclQ0nH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_moons\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# Create dataset\n",
        "X, y = make_moons(n_samples=200, noise=0.2, random_state=42)\n",
        "\n",
        "# Train SVM classifier with polynomial kernel\n",
        "svm_poly = SVC(kernel='poly', degree=3)\n",
        "svm_poly.fit(X, y)\n",
        "\n",
        "# Plot decision boundary\n",
        "xx, yy = np.meshgrid(np.linspace(X[:, 0].min(), X[:, 0].max(), 100),\n",
        "                     np.linspace(X[:, 1].min(), X[:, 1].max(), 100))\n",
        "Z = svm_poly.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "Z = Z.reshape(xx.shape)\n",
        "\n",
        "plt.contourf(xx, yy, Z, alpha=0.3)\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k')\n",
        "plt.title(\"SVM with Polynomial Kernel\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "mAPNVlMeQ5O7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q25 - Write a Python program to train a Gaussian Naïve Bayes classifier on the Breast Cancer dataset and evaluate accuracy."
      ],
      "metadata": {
        "id": "AMGSeed8Q8XA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "breast_cancer = datasets.load_breast_cancer()\n",
        "X, y = breast_cancer.data, breast_cancer.target\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Gaussian Naïve Bayes classifier\n",
        "gnb = GaussianNB()\n",
        "gnb.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate accuracy\n",
        "y_pred = gnb.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")"
      ],
      "metadata": {
        "id": "5V41u3SrRAmp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q26 - Write a Python program to train a Multinomial Naïve Bayes classifier for text classification using the 20 Newsgroups dataset."
      ],
      "metadata": {
        "id": "cDPAmkqJRGvd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "newsgroups = fetch_20newsgroups(subset='all', categories=['sci.space', 'comp.graphics'])\n",
        "X, y = newsgroups.data, newsgroups.target\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create pipeline for text processing and classification\n",
        "text_clf = Pipeline([\n",
        "    ('vect', CountVectorizer()),\n",
        "    ('tfidf', TfidfTransformer()),\n",
        "    ('clf', MultinomialNB())\n",
        "])\n",
        "\n",
        "# Train classifier\n",
        "text_clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate accuracy\n",
        "y_pred = text_clf.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")"
      ],
      "metadata": {
        "id": "DYota5N5RIQK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q27 - Train an SVM Classifier with different C values and compare decision boundaries visually."
      ],
      "metadata": {
        "id": "zyAid9T0RLRA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# Generate synthetic data\n",
        "X, y = make_classification(n_samples=100, n_features=2, n_classes=2, n_clusters_per_class=1, random_state=42)\n",
        "\n",
        "# Define different C values\n",
        "C_values = [0.1, 1, 10]\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "for idx, C in enumerate(C_values):\n",
        "    svm_clf = SVC(kernel='linear', C=C)\n",
        "    svm_clf.fit(X, y)\n",
        "\n",
        "    # Plot decision boundary\n",
        "    xx, yy = np.meshgrid(np.linspace(X[:, 0].min(), X[:, 0].max(), 100),\n",
        "                         np.linspace(X[:, 1].min(), X[:, 1].max(), 100))\n",
        "    Z = svm_clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
        "    Z = Z.reshape(xx.shape)\n",
        "\n",
        "    axes[idx].contourf(xx, yy, Z, levels=[Z.min(), 0, Z.max()], alpha=0.2)\n",
        "    axes[idx].scatter(X[:, 0], X[:, 1], c=y, edgecolors='k')\n",
        "    axes[idx].set_title(f\"SVM with C={C}\")\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1pJnfrMiRSq_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q28 - Train a Bernoulli Naïve Bayes classifier for binary classification on a dataset with binary features."
      ],
      "metadata": {
        "id": "S5ExKJnsRVKE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import BernoulliNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Generate binary dataset\n",
        "X, y = make_classification(n_samples=200, n_features=10, n_classes=2, random_state=42)\n",
        "X = (X > 0).astype(int)  # Convert to binary features\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train BernoulliNB classifier\n",
        "bnb = BernoulliNB()\n",
        "bnb.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate accuracy\n",
        "y_pred = bnb.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")"
      ],
      "metadata": {
        "id": "Q6VXP7DLRcF3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q29 - Apply feature scaling before training an SVM model and compare results with unscaled data."
      ],
      "metadata": {
        "id": "3KG99YH5Re64"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# Train SVM without scaling\n",
        "svm_unscaled = SVC(kernel='rbf')\n",
        "svm_unscaled.fit(X_train, y_train)\n",
        "accuracy_unscaled = accuracy_score(y_test, svm_unscaled.predict(X_test))\n",
        "\n",
        "# Train SVM with scaling\n",
        "pipeline = Pipeline([('scaler', StandardScaler()), ('svm', SVC(kernel='rbf'))])\n",
        "pipeline.fit(X_train, y_train)\n",
        "accuracy_scaled = accuracy_score(y_test, pipeline.predict(X_test))\n",
        "\n",
        "print(f\"Unscaled Accuracy: {accuracy_unscaled:.2f}\")\n",
        "print(f\"Scaled Accuracy: {accuracy_scaled:.2f}\")"
      ],
      "metadata": {
        "id": "Lg_rNe3JRkl4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q30 - Train a Gaussian Naïve Bayes model and compare predictions before and after Laplace Smoothing."
      ],
      "metadata": {
        "id": "zNEYhC9uRn7T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "# Train GaussianNB without Laplace smoothing\n",
        "gnb_no_smooth = GaussianNB(var_smoothing=1e-9)  # Default smoothing\n",
        "gnb_no_smooth.fit(X_train, y_train)\n",
        "accuracy_no_smooth = accuracy_score(y_test, gnb_no_smooth.predict(X_test))\n",
        "\n",
        "# Train GaussianNB with increased Laplace smoothing\n",
        "gnb_smooth = GaussianNB(var_smoothing=1e-2)\n",
        "gnb_smooth.fit(X_train, y_train)\n",
        "accuracy_smooth = accuracy_score(y_test, gnb_smooth.predict(X_test))\n",
        "\n",
        "print(f\"Accuracy without Laplace Smoothing: {accuracy_no_smooth:.2f}\")\n",
        "print(f\"Accuracy with Laplace Smoothing: {accuracy_smooth:.2f}\")"
      ],
      "metadata": {
        "id": "DiK95gbYRrwK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q31 - Train an SVM Classifier and use GridSearchCV to tune hyperparameters (C, gamma, kernel)."
      ],
      "metadata": {
        "id": "X9Z4qqxLRuqY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Define parameter grid\n",
        "param_grid = {'C': [0.1, 1, 10], 'gamma': ['scale', 'auto'], 'kernel': ['linear', 'rbf']}\n",
        "\n",
        "# Grid search with cross-validation\n",
        "grid_search = GridSearchCV(SVC(), param_grid, cv=5)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best parameters and accuracy\n",
        "print(f\"Best Parameters: {grid_search.best_params_}\")\n",
        "print(f\"Best Accuracy: {grid_search.best_score_:.2f}\")"
      ],
      "metadata": {
        "id": "WPWZ0HlJRzjc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q32 - Train an SVM Classifier on an imbalanced dataset and apply class weighting."
      ],
      "metadata": {
        "id": "_8JHkOA0R4PP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "# Generate imbalanced dataset\n",
        "X_imb, y_imb = make_classification(n_samples=1000, weights=[0.9, 0.1], n_classes=2, random_state=42)\n",
        "\n",
        "# Compute class weights\n",
        "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_imb), y=y_imb)\n",
        "class_weight_dict = {i: weight for i, weight in enumerate(class_weights)}\n",
        "\n",
        "# Train SVM with class weighting\n",
        "svm_weighted = SVC(kernel='rbf', class_weight=class_weight_dict)\n",
        "svm_weighted.fit(X_train, y_train)\n",
        "accuracy_weighted = accuracy_score(y_test, svm_weighted.predict(X_test))\n",
        "\n",
        "print(f\"Weighted SVM Accuracy: {accuracy_weighted:.2f}\")"
      ],
      "metadata": {
        "id": "bDhceyd5R8CG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q33 - Implement a Naïve Bayes classifier for spam detection using email data."
      ],
      "metadata": {
        "id": "pBXF0xPRR9Nl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Example email dataset (use actual spam dataset if available)\n",
        "emails = [\"Free money now!!!\", \"Hey, are we meeting tomorrow?\", \"Limited time offer, claim now!\", \"Project update attached\"]\n",
        "labels = [1, 0, 1, 0]  # 1 = spam, 0 = not spam\n",
        "\n",
        "# Train Naïve Bayes classifier\n",
        "spam_clf = Pipeline([\n",
        "    ('vect', CountVectorizer()),\n",
        "    ('tfidf', TfidfTransformer()),\n",
        "    ('clf', MultinomialNB())\n",
        "])\n",
        "spam_clf.fit(emails, labels)\n",
        "\n",
        "# Test on new email\n",
        "print(spam_clf.predict([\"Win a free iPhone!\"]))"
      ],
      "metadata": {
        "id": "WUl5hsIWSCO8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q34 - Train an SVM Classifier and a Naïve Bayes Classifier on the same dataset and compare accuracy."
      ],
      "metadata": {
        "id": "72BDdS5hSFLu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "# Train SVM\n",
        "svm_clf = SVC(kernel='rbf')\n",
        "svm_clf.fit(X_train, y_train)\n",
        "accuracy_svm = accuracy_score(y_test, svm_clf.predict(X_test))\n",
        "\n",
        "# Train Naïve Bayes\n",
        "nb_clf = GaussianNB()\n",
        "nb_clf.fit(X_train, y_train)\n",
        "accuracy_nb = accuracy_score(y_test, nb_clf.predict(X_test))\n",
        "\n",
        "print(f\"SVM Accuracy: {accuracy_svm:.2f}\")\n",
        "print(f\"Naïve Bayes Accuracy: {accuracy_nb:.2f}\")"
      ],
      "metadata": {
        "id": "MI6iAZ6rSKGQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q35 - Perform feature selection before training a Naïve Bayes classifier and compare results."
      ],
      "metadata": {
        "id": "fFLDJ5pZSN4U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_selection import SelectKBest, chi2\n",
        "\n",
        "# Feature selection\n",
        "selector = SelectKBest(chi2, k=5)  # Select top 5 features\n",
        "X_train_selected = selector.fit_transform(X_train, y_train)\n",
        "X_test_selected = selector.transform(X_test)\n",
        "\n",
        "# Train Naïve Bayes classifier with selected features\n",
        "nb_selected = GaussianNB()\n",
        "nb_selected.fit(X_train_selected, y_train)\n",
        "accuracy_selected = accuracy_score(y_test, nb_selected.predict(X_test_selected))\n",
        "\n",
        "print(f\"Accuracy with Feature Selection: {accuracy_selected:.2f}\")"
      ],
      "metadata": {
        "id": "4HVJLByaSTWL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q36 - Train an SVM Classifier using One-vs-Rest (OvR) and One-vs-One (OvO) on the Wine dataset."
      ],
      "metadata": {
        "id": "0y0oC-cGSVxb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.multiclass import OneVsRestClassifier, OneVsOneClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Wine dataset\n",
        "wine = datasets.load_wine()\n",
        "X, y = wine.data, wine.target\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train One-vs-Rest (OvR)\n",
        "ovr_clf = OneVsRestClassifier(SVC(kernel='linear'))\n",
        "ovr_clf.fit(X_train, y_train)\n",
        "ovr_acc = accuracy_score(y_test, ovr_clf.predict(X_test))\n",
        "\n",
        "# Train One-vs-One (OvO)\n",
        "ovo_clf = OneVsOneClassifier(SVC(kernel='linear'))\n",
        "ovo_clf.fit(X_train, y_train)\n",
        "ovo_acc = accuracy_score(y_test, ovo_clf.predict(X_test))\n",
        "\n",
        "print(f\"OvR Accuracy: {ovr_acc:.2f}\")\n",
        "print(f\"OvO Accuracy: {ovo_acc:.2f}\")"
      ],
      "metadata": {
        "id": "ejIkdFf3TLqY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q37 - Train an SVM Classifier using Linear, Polynomial, and RBF kernels on the Breast Cancer dataset."
      ],
      "metadata": {
        "id": "z6TLkkOkTOnJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# Load dataset\n",
        "cancer = load_breast_cancer()\n",
        "X, y = cancer.data, cancer.target\n",
        "\n",
        "# Train and compare kernels\n",
        "kernels = ['linear', 'poly', 'rbf']\n",
        "for kernel in kernels:\n",
        "    svm_clf = SVC(kernel=kernel)\n",
        "    svm_clf.fit(X_train, y_train)\n",
        "    acc = accuracy_score(y_test, svm_clf.predict(X_test))\n",
        "    print(f\"{kernel.capitalize()} Kernel Accuracy: {acc:.2f}\")"
      ],
      "metadata": {
        "id": "dzr6hJNXTSkd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q38 - Train an SVM Classifier using Stratified K-Fold Cross-Validation and compute average accuracy."
      ],
      "metadata": {
        "id": "cVjmbdQoTaQA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "\n",
        "# Stratified K-Fold\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "svm_clf = SVC(kernel='linear')\n",
        "\n",
        "# Cross-validation accuracy\n",
        "scores = cross_val_score(svm_clf, X, y, cv=skf, scoring='accuracy')\n",
        "print(f\"Average Accuracy: {scores.mean():.2f}\")"
      ],
      "metadata": {
        "id": "t9ePHz3ZTx6i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q39 - Train a Naïve Bayes classifier using different prior probabilities and compare performance."
      ],
      "metadata": {
        "id": "cl5140IqTz9l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "# Define different priors\n",
        "priors_list = [[0.7, 0.3], [0.5, 0.5], [0.3, 0.7]]\n",
        "\n",
        "for priors in priors_list:\n",
        "    nb_clf = GaussianNB(priors=priors)\n",
        "    nb_clf.fit(X_train, y_train)\n",
        "    acc = accuracy_score(y_test, nb_clf.predict(X_test))\n",
        "    print(f\"Prior {priors} - Accuracy: {acc:.2f}\")"
      ],
      "metadata": {
        "id": "VyzKrCbkT6M0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q40 - Perform Recursive Feature Elimination (RFE) before training an SVM Classifier."
      ],
      "metadata": {
        "id": "jzD3LJSnT7TS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_selection import RFE\n",
        "\n",
        "# Perform RFE\n",
        "svm_clf = SVC(kernel='linear')\n",
        "rfe = RFE(estimator=svm_clf, n_features_to_select=5)\n",
        "X_train_rfe = rfe.fit_transform(X_train, y_train)\n",
        "X_test_rfe = rfe.transform(X_test)\n",
        "\n",
        "# Train SVM with selected features\n",
        "svm_clf.fit(X_train_rfe, y_train)\n",
        "acc_rfe = accuracy_score(y_test, svm_clf.predict(X_test_rfe))\n",
        "\n",
        "print(f\"Accuracy after RFE: {acc_rfe:.2f}\")"
      ],
      "metadata": {
        "id": "tp5l0TdpT_7K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q41 - Train an SVM Classifier and evaluate using Precision, Recall, and F1-Score."
      ],
      "metadata": {
        "id": "FEuvhZOSUCcZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "# Train SVM\n",
        "svm_clf = SVC(kernel='rbf')\n",
        "svm_clf.fit(X_train, y_train)\n",
        "y_pred = svm_clf.predict(X_test)\n",
        "\n",
        "# Compute metrics\n",
        "precision = precision_score(y_test, y_pred, average='weighted')\n",
        "recall = recall_score(y_test, y_pred, average='weighted')\n",
        "f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "print(f\"Precision: {precision:.2f}, Recall: {recall:.2f}, F1-Score: {f1:.2f}\")"
      ],
      "metadata": {
        "id": "biyz5HErUIZc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q42 - Train a Naïve Bayes Classifier and evaluate using Log Loss (Cross-Entropy Loss)."
      ],
      "metadata": {
        "id": "XLWNJl2aUMFj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import log_loss\n",
        "\n",
        "# Train Naïve Bayes\n",
        "nb_clf = GaussianNB()\n",
        "nb_clf.fit(X_train, y_train)\n",
        "\n",
        "# Compute Log Loss\n",
        "y_prob = nb_clf.predict_proba(X_test)\n",
        "log_loss_value = log_loss(y_test, y_prob)\n",
        "\n",
        "print(f\"Log Loss: {log_loss_value:.2f}\")"
      ],
      "metadata": {
        "id": "yMejFosLUQbZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q43 - Train an SVM Classifier and visualize the Confusion Matrix using seaborn."
      ],
      "metadata": {
        "id": "_wcU_wLYUS0_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Compute Confusion Matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Plot using seaborn\n",
        "plt.figure(figsize=(5,4))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=cancer.target_names, yticklabels=cancer.target_names)\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "uMTSRstLUXZW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q44 - Train an SVM Regressor (SVR) and evaluate using Mean Absolute Error (MAE)."
      ],
      "metadata": {
        "id": "_iDrX4idUawa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.datasets import make_regression\n",
        "\n",
        "# Generate regression data\n",
        "X_reg, y_reg = make_regression(n_samples=200, n_features=5, noise=0.1, random_state=42)\n",
        "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(X_reg, y_reg, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train SVR\n",
        "svr = SVR(kernel='rbf')\n",
        "svr.fit(X_train_reg, y_train_reg)\n",
        "\n",
        "# Compute MAE\n",
        "y_pred_reg = svr.predict(X_test_reg)\n",
        "mae = mean_absolute_error(y_test_reg, y_pred_reg)\n",
        "\n",
        "print(f\"Mean Absolute Error (MAE): {mae:.2f}\")"
      ],
      "metadata": {
        "id": "qV1GVZtKUftr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q45 - Train a Naïve Bayes classifier and evaluate its performance using the ROC-AUC score."
      ],
      "metadata": {
        "id": "yUhEvX1pUik_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.preprocessing import label_binarize\n",
        "\n",
        "# Train Naïve Bayes\n",
        "nb_clf = GaussianNB()\n",
        "nb_clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities\n",
        "y_prob = nb_clf.predict_proba(X_test)\n",
        "\n",
        "# Binarize labels for multi-class classification\n",
        "y_test_bin = label_binarize(y_test, classes=[0, 1, 2])\n",
        "\n",
        "# Compute ROC-AUC score for each class and average\n",
        "roc_auc = roc_auc_score(y_test_bin, y_prob, multi_class=\"ovr\")\n",
        "\n",
        "print(f\"ROC-AUC Score: {roc_auc:.2f}\")"
      ],
      "metadata": {
        "id": "WaqegcQSUoxk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q46 - Train an SVM Classifier and visualize the Precision-Recall Curve."
      ],
      "metadata": {
        "id": "H70_LAJYUrnH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import precision_recall_curve, auc\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# Train SVM Classifier\n",
        "svm_clf = SVC(kernel='rbf', probability=True)\n",
        "svm_clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities\n",
        "y_prob = svm_clf.predict_proba(X_test)[:, 1]  # Taking probability of class 1\n",
        "\n",
        "# Compute Precision-Recall curve\n",
        "precision, recall, _ = precision_recall_curve(y_test, y_prob)\n",
        "\n",
        "# Compute AUC\n",
        "pr_auc = auc(recall, precision)\n",
        "\n",
        "# Plot Precision-Recall Curve\n",
        "plt.plot(recall, precision, marker='.', label=f'PR AUC = {pr_auc:.2f}')\n",
        "plt.xlabel(\"Recall\")\n",
        "plt.ylabel(\"Precision\")\n",
        "plt.title(\"Precision-Recall Curve\")\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "LK5Yqa-dUw58"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}